import os
import sys
import json
import torch
from safetensors.torch import load_file

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # ä¿æŒä¸å˜
from finetune.transformer.transformer import RoboMultiTransformerModel

# ====== é…ç½® ======
model_dir = "/disk0/home/kuowei/Robo_MLLM_v1/transformer"
transformer_model_path = model_dir
safetensors_file = os.path.join(model_dir, "diffusion_pytorch_model-00001-of-00005.safetensors")  # å…¶ä¸­ä¸€ä¸ªæƒé‡æ–‡ä»¶
index_file = os.path.join(model_dir, "diffusion_pytorch_model.safetensors.index.json")
# ==================

# Step 1. æ£€æŸ¥ checkpoint index é‡Œæ˜¯å¦æœ‰ action_transformer çš„ key
print("\nğŸ“¦ Step 1: æ£€æŸ¥ checkpoint index é‡Œæ˜¯å¦æœ‰ action_transformer keys")
with open(index_file, "r") as f:
    index = json.load(f)

keys = index["weight_map"].keys()
action_keys = [key for key in keys if "action_transformer" in key]

if action_keys:
    print(f"âœ… æ‰¾åˆ° {len(action_keys)} ä¸ª action_transformer æƒé‡åï¼š")
    for key in action_keys:
        print(f"   - {key}")
else:
    print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• action_transformer æƒé‡ï¼")
    exit(1)

# Step 2. åŠ è½½ä¸€ä¸ª safetensors æ–‡ä»¶ï¼Œæ£€æŸ¥ action_transformer çš„æƒé‡æ˜¯ä¸æ˜¯æ­£å¸¸çš„ Tensor
print("\nğŸ“¦ Step 2: æ£€æŸ¥ safetensors æ–‡ä»¶ä¸­æ˜¯å¦å­˜åœ¨ action_transformer çš„æ•°æ®")

# æŸ¥æ‰¾æ‰€æœ‰ safetensors æ–‡ä»¶
all_files = os.listdir(model_dir)
safetensors_files = [f for f in all_files if f.endswith(".safetensors")]

if not safetensors_files:
    raise FileNotFoundError(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .safetensors æ–‡ä»¶ in {model_dir}ï¼")
else:
    print(f"ğŸ” æ‰¾åˆ°ä»¥ä¸‹ safetensors æ–‡ä»¶:")
    for f in safetensors_files:
        print(f"   - {f}")

# æ£€æŸ¥æ‰€æœ‰ safetensors æ–‡ä»¶ä¸­æ˜¯å¦å­˜åœ¨ action_transformer çš„æƒé‡
# æ£€æŸ¥æ‰€æœ‰ safetensors æ–‡ä»¶ä¸­æ˜¯å¦å­˜åœ¨ action_transformer çš„æƒé‡
found_action_transformer = False
for safetensors_file in safetensors_files:
    safetensors_path = os.path.join(model_dir, safetensors_file)
    
    try:
        # åŠ è½½ safetensors æ–‡ä»¶
        tensor_file = load_file(safetensors_path)
        
        # æŸ¥æ‰¾åŒ…å« "action_transformer" çš„é”®
        tensor_keys = tensor_file.keys()
        found_keys = [k for k in tensor_keys if "action_transformer" in k]
        
        if found_keys:
            found_action_transformer = True
            print(f"\nâœ… åœ¨ {safetensors_file} é‡Œæ‰¾åˆ°äº† {len(found_keys)} ä¸ª action_transformer tensorï¼š")
            for key in found_keys:
                print(f"   - {key}")
                
                # è·å–å¯¹åº”çš„ tensor å€¼å¹¶æ‰“å°å‡ºæ¥
                tensor_value = tensor_file[key]
                
                # æ£€æŸ¥ tensor æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸”è¾“å‡ºå…¶æ•°å€¼
                if isinstance(tensor_value, torch.Tensor):
                    print(f"   âœ… tensor çš„å½¢çŠ¶ä¸º: {tensor_value.shape}")
                    print(f"   âœ… tensor çš„æ•°å€¼ï¼š\n{tensor_value}")
                else:
                    print(f"   âŒ {key} ä¸æ˜¯æœ‰æ•ˆçš„ tensorï¼Œæ— æ³•è¾“å‡ºæ•°å€¼")
        else:
            print(f"âŒ {safetensors_file} ä¸­æ²¡æœ‰ä»»ä½• action_transformer çš„ tensorï¼")
    
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ {safetensors_file} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

if found_action_transformer:
    print("\nâœ… action_transformer æƒé‡å·²æˆåŠŸæ‰¾åˆ°ï¼")
else:
    print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• action_transformer æƒé‡ï¼")

# Step 3. æ­£ç¡®åŠ è½½ transformer model
print("\nğŸš€ Step 3: åŠ è½½ RoboMultiTransformerModel from_pretrained...")
model = RoboMultiTransformerModel.from_pretrained(
    transformer_model_path,
    subfolder="",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=False,
    device_map=None,
)

# Step 4. æ£€æŸ¥ action_transformer é‡Œé¢æœ‰æ²¡æœ‰ meta tensor
print("\nğŸ” Step 4: æ£€æŸ¥ action_transformer æ¨¡å—çš„å‚æ•° device å’Œæ•°å€¼")

meta_params = []
normal_params = []

# æŸ¥çœ‹æ¨¡å‹ä¸­çš„æ‰€æœ‰ action_transformer å‚æ•°
for n, p in model.action_transformer.named_parameters():
    if p.device.type == "meta":
        meta_params.append(n)
    else:
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ•°å€¼ï¼ˆå³émetaï¼Œå¹¶ä¸”å…¶å€¼æ˜¯å¦ä¸º NaNï¼‰
        if torch.isnan(p).any():
            print(f"âŒ {n} å«æœ‰ NaN æ•°å€¼ï¼")
        else:
            normal_params.append(n)

if normal_params:
    print(f"âœ… æ­£å¸¸åŠ è½½äº† {len(normal_params)} ä¸ªå‚æ•°ï¼Œæ¯”å¦‚ï¼š")
    for name in normal_params[:5]:  # åªåˆ—ä¸€éƒ¨åˆ†
        print(f"   - {name}")
if meta_params:
    print(f"\nâŒ å‘ç° {len(meta_params)} ä¸ª meta tensorï¼Œåˆ—è¡¨å¦‚ä¸‹ï¼š")
    for name in meta_params:
        print(f"   - {name}")
    print("\nâš ï¸ æ¨ç†æ—¶ä¸€å®šä¼šå´©æºƒï¼Œéœ€è¦ materialize è¿™äº›å‚æ•°åˆ° deviceï¼")

    # è¿™é‡Œåš materialize æ“ä½œ
    print("\nğŸ”§ è¿›è¡Œ materialize æ“ä½œ...")
    for name in meta_params:
        param = dict(model.action_transformer.named_parameters())[name]
        if param.device.type == "meta":
            with torch.no_grad():
                # ä½¿ç”¨ to_empty() è½¬ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
                param.data = param.to_empty(device=param.device)
                print(f"âœ… å·²å°† {name} è½¬ç§»åˆ°è®¾å¤‡ {param.device}")
else:
    print("\nâœ… action_transformer æ‰€æœ‰å‚æ•°éƒ½åœ¨æ­£å¸¸ device ä¸Šï¼Œæ¨ç†å¯ä»¥ç»§ç»­ï¼")

print("\nğŸ¯ æ£€æŸ¥å®Œæˆï¼")
